{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import path\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynteticSet(data.Dataset):\n",
    "    def __init__(self, df, features, target=\"target\"):\n",
    "        target_dict = {\"Class_1\": 0, \"Class_2\": 1, \"Class_3\": 2, \"Class_4\": 3}\n",
    "        \n",
    "        for i in target_dict:\n",
    "            df[target][df[target] == i] = target_dict[i]\n",
    "           \n",
    "        prefix = \"feature_{}\"\n",
    "        \n",
    "        for i,feature in enumerate(features):\n",
    "            features[i] = prefix.format(str(feature))\n",
    "        \n",
    "            \n",
    "        self.X = torch.tensor(df[features].to_numpy()).float()\n",
    "        self.Y = torch.tensor(df[target].to_numpy(dtype=np.int64))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_path = path.Path(\".\\\\data\")\n",
    "df = pd.read_csv(data_path/\"train.csv\")\n",
    "    \n",
    "dataset = SynteticSet(df, [i for i in range(3)])\n",
    "dataset_len = len(dataset)\n",
    "\n",
    "trainset, evalset = data.random_split(dataset, \n",
    "                                      [int(dataset_len*0.7), int(dataset_len*0.3)])\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "evalloader = data.DataLoader(evalset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(3,4),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN()\n",
    "lossF = nn.CrossEntropyLoss()\n",
    "\n",
    "opt = SGD(net.parameters() ,lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    loss_list = list()\n",
    "    for x,y in trainloader:\n",
    "\n",
    "        opt.zero_grad()\n",
    "        result = net(x)\n",
    "        loss = lossF(result, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    print(np.array(loss_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3459895003648301\n",
      "1.2228841583297283\n",
      "1.1894371132528105\n",
      "1.1815714343592278\n",
      "1.1778991294299446\n",
      "1.1757248936449152\n",
      "1.1742904109972268\n",
      "1.1733017299031427\n",
      "1.1725233627629672\n",
      "1.1719526268228535\n",
      "1.1714781481343606\n",
      "1.171114304819752\n",
      "1.1707771892303522\n",
      "1.1705005421716945\n",
      "1.1702955579844863\n",
      "1.1701039596711043\n",
      "1.169923316408119\n",
      "1.1697650565961597\n",
      "1.16960595752465\n",
      "1.1695128892849742\n",
      "1.1693981820530168\n",
      "1.1692661291066646\n",
      "1.1691868194060526\n",
      "1.169121674471407\n",
      "1.1690293254102606\n",
      "1.168966626990212\n",
      "1.1689057844866346\n",
      "1.1688294787729463\n",
      "1.1687814912586805\n",
      "1.1687233110232589\n",
      "1.1686686216368318\n",
      "1.1686250843121322\n",
      "1.1685823337251569\n",
      "1.1685561002717375\n",
      "1.1685102310651398\n",
      "1.1684744397290663\n",
      "1.1684608601129034\n",
      "1.1684188539846723\n",
      "1.1683905684751827\n",
      "1.1683577403725829\n",
      "1.1683283170354868\n",
      "1.168322537455306\n",
      "1.168271646203245\n",
      "1.1682499300627212\n",
      "1.1682193115816693\n",
      "1.1682077426143276\n",
      "1.168181063488153\n",
      "1.1681773808582174\n",
      "1.1681624350207815\n",
      "1.1681320789309262\n",
      "1.1680943479921526\n",
      "1.1681140018337606\n",
      "1.1680754991946316\n",
      "1.1680741135773318\n",
      "1.1680509970000719\n",
      "1.1680386007594887\n",
      "1.1680205953622431\n",
      "1.1680151727561323\n",
      "1.1679900712461315\n",
      "1.1679714231648\n",
      "1.1679714170626792\n",
      "1.1679454889332355\n",
      "1.1679401155799574\n",
      "1.1679271193701344\n",
      "1.1679285182813404\n",
      "1.1679121131652888\n",
      "1.1679020529473285\n",
      "1.167892255120565\n",
      "1.1678706455492232\n",
      "1.1678594363890793\n",
      "1.1678626820635751\n",
      "1.1678519349211531\n",
      "1.1678474696942178\n",
      "1.167841357984316\n",
      "1.1678352582607234\n",
      "1.167825526467625\n",
      "1.1678219815712743\n",
      "1.1678206754994784\n",
      "1.1678074179881233\n",
      "1.1677942705328765\n",
      "1.1677914053692025\n",
      "1.167776625596627\n",
      "1.1677681153173638\n",
      "1.1677657779871438\n",
      "1.1677656437404849\n",
      "1.1677778488539254\n",
      "1.167729253742769\n",
      "1.1677478846074023\n",
      "1.1677481894955122\n",
      "1.1677326578544742\n",
      "1.1677311909482073\n",
      "1.1677197559006254\n",
      "1.1677146210748928\n",
      "1.167695301433368\n",
      "1.167710614596686\n",
      "1.1677200033634192\n",
      "1.167692900575711\n",
      "1.1677003730585631\n",
      "1.1676958139025533\n",
      "1.1677014993357266\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.17243492603302  Accuracy: 0.571566641330719\n"
     ]
    }
   ],
   "source": [
    "x, y = evalset[:]\n",
    "result = net(x[:, 0:4])\n",
    "accuracy = (result.argmax(dim=1) == y).float().mean()\n",
    "loss = lossF(result, y)\n",
    "\n",
    "print(f\"Loss: {loss}  Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
