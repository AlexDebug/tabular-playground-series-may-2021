{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import path\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynteticSet(data.Dataset):\n",
    "    def __init__(self, df, features, target=\"target\"):\n",
    "        target_dict = {\"Class_1\": 0, \"Class_2\": 1, \"Class_3\": 2, \"Class_4\": 3}\n",
    "        \n",
    "        for i in target_dict:\n",
    "            df[target][df[target] == i] = target_dict[i]\n",
    "           \n",
    "        prefix = \"feature_{}\"\n",
    "        \n",
    "        for i,feature in enumerate(features):\n",
    "            features[i] = prefix.format(str(feature))\n",
    "        \n",
    "            \n",
    "        self.X = torch.tensor(df[features].to_numpy()).float()\n",
    "        self.Y = torch.tensor(df[target].to_numpy(dtype=np.int64))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_path = path.Path(\".\\\\data\")\n",
    "df = pd.read_csv(data_path/\"train.csv\")\n",
    "    \n",
    "dataset = SynteticSet(df, [i for i in range(50)])\n",
    "dataset_len = len(dataset)\n",
    "\n",
    "trainset, evalset = data.random_split(dataset, \n",
    "                                      [int(dataset_len*0.7), int(dataset_len*0.3)])\n",
    "\n",
    "trainloader = data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "evalloader = data.DataLoader(evalset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(50,4),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN()\n",
    "lossF = nn.CrossEntropyLoss()\n",
    "\n",
    "opt = SGD(net.parameters() ,lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    loss_list = list()\n",
    "    for x,y in trainloader:\n",
    "\n",
    "        opt.zero_grad()\n",
    "        result = net(x)\n",
    "        loss = lossF(result, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    print(np.array(loss_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1864185775655716\n",
      "1.1724845391086949\n",
      "1.1711962151135125\n",
      "1.1706126762700473\n",
      "1.1702739403496276\n",
      "1.1700367334773816\n",
      "1.1698684208571801\n",
      "1.1697379520649884\n",
      "1.1696465431663192\n",
      "1.169572962703391\n",
      "1.1694952735517317\n",
      "1.169459666586881\n",
      "1.1694219764015574\n",
      "1.1693691546999776\n",
      "1.1693321469714917\n",
      "1.1693049126612858\n",
      "1.1692762845612967\n",
      "1.169243421153569\n",
      "1.169235057761072\n",
      "1.1692045432340075\n",
      "1.1691774594064603\n",
      "1.169164233713307\n",
      "1.1691559413669315\n",
      "1.1691354687514646\n",
      "1.1691038410232097\n",
      "1.1691267008755282\n",
      "1.169106463191714\n",
      "1.1691113285434311\n",
      "1.1690593519855896\n",
      "1.1690733027414604\n",
      "1.1690680527381967\n",
      "1.1690636236881207\n",
      "1.1690361347233356\n",
      "1.1690323492290768\n",
      "1.169016893428684\n",
      "1.1690223319438955\n",
      "1.1690217250008035\n",
      "1.1690397704977005\n",
      "1.1690034447901863\n",
      "1.1690098871042784\n",
      "1.168990465361196\n",
      "1.1689771538024847\n",
      "1.168988569998872\n",
      "1.1689793187477906\n",
      "1.168957561200018\n",
      "1.1689502231817577\n",
      "1.1689745174683426\n",
      "1.1689627706677648\n",
      "1.1689549741187086\n",
      "1.1689504021046584\n",
      "1.168971385336881\n",
      "1.1689541361667557\n",
      "1.1689399753235812\n",
      "1.1689473072397207\n",
      "1.1689441788131183\n",
      "1.168929987023474\n",
      "1.1689362414794169\n",
      "1.168912459117185\n",
      "1.1689362229551215\n",
      "1.168947784294812\n",
      "1.1689268505333545\n",
      "1.1689138900645255\n",
      "1.168917681660905\n",
      "1.1689092186730785\n",
      "1.1689067296615685\n",
      "1.1688981167359151\n",
      "1.168901492734278\n",
      "1.1688880092265184\n",
      "1.1688919522862549\n",
      "1.1689015053743854\n",
      "1.1688810730765027\n",
      "1.1689173449545935\n",
      "1.1689014681078616\n",
      "1.1688830808921968\n",
      "1.1688834522498373\n",
      "1.1688768511716365\n",
      "1.1688916598203196\n",
      "1.1689002620672613\n",
      "1.1688821681892632\n",
      "1.1688599612638764\n",
      "1.1688684190213134\n",
      "1.1688747883278883\n",
      "1.1688763559191493\n",
      "1.1688573744004997\n",
      "1.1688681723212848\n",
      "1.1688841980161788\n",
      "1.1688747994424655\n",
      "1.1688529054687053\n",
      "1.1688735155998244\n",
      "1.1688700291094876\n",
      "1.1688699037980772\n",
      "1.1688527725296438\n",
      "1.1688751184962132\n",
      "1.168861329228612\n",
      "1.168880958225871\n",
      "1.1688484005779844\n",
      "1.1688492860393072\n",
      "1.168849912814292\n",
      "1.1688481634670027\n",
      "1.1688344038382745\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.17243492603302  Accuracy: 0.571566641330719\n"
     ]
    }
   ],
   "source": [
    "x, y = evalset[:]\n",
    "result = net(x[:, 0:4])\n",
    "accuracy = (result.argmax(dim=1) == y).float().mean()\n",
    "loss = lossF(result, y)\n",
    "\n",
    "print(f\"Loss: {loss}  Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
